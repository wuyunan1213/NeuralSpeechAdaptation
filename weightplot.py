# -*- coding: utf-8 -*-
"""nn_tests.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x6xrVQEnTm9Slj9k_T_TlbTZQQu1quNe
"""


import os
import pickle as pkl
import numpy as np
import datetime
import random
import keras
#import tensorflow as tf
import matplotlib.pyplot as plt
from math import ceil
from keras_lr_multiplier import LRMultiplier
#from tensorflow.keras.models import Sequential, Model
from keras.models import Sequential, Model
#from tensorflow.keras.layers import Input, Dense, Dropout, LSTM, BatchNormalization
from keras.layers import Input, Dense, Dropout
#from tensorflow.keras.callbacks import TensorBoard
from keras.callbacks import LambdaCallback, ModelCheckpoint
from mpl_toolkits.mplot3d import Axes3D
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
from LR_SGD import LR_SGD





'''Loading Data Functions'''
def load_data(name, folder = None):
    if folder is None:
        file_name = "".join((os.getcwd(), "//", name))
    else:
        file_name = "".join((os.getcwd(), "//", folder, "//", name))
    file = open(file_name, 'rb')
    data = pkl.load(file)
    file.close()
    return data

def loadOfInt(names):
    if type(names) == str:
        return load_data(names)
    all_dfs = []
    for n in names:
        df = load_data(n)
        all_dfs.append(df)
    return all_dfs

def ff_nn(n_slow=30):
    model = Sequential()
    model.add(Dense(n_slow, activation="tanh", use_bias=True, name="slow"))
    model.add(Dense(1, activation='sigmoid', use_bias=True, name="slow_out"))
    # opt = tf.keras.optimizers.Adam(lr = 0.002)#, decay = 1e-6)
    opt = keras.optimizers.Adam(lr = 0.002)
    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])
    return model

# kernel_initializer="zeros", bias_initializer='zeros' ?
def slow_fast_nn(lr_s = 1, lr_f=18, n_inp = 30, n_slow=30, n_fast=30, train_slow = True):
    inp = Input(shape = (n_inp,), name = 'input')
    
    slow = Dense(n_slow, activation="tanh", name = 'slow')(inp)
    fast = Dense(n_fast, activation="tanh", name = 'fast')(inp)
    
    #To be able to assign a different learning rate to the entire path in fast or slow
    s2 = Dense(1, activation="linear", name = 'slow_2')(slow)
    f2 = Dense(1, activation="linear", name = 'fast_2')(fast)
    added = keras.layers.Add()([s2, f2])
    out = Dense(1, activation="sigmoid", name="output")(added)
    model = Model(inputs = inp, outputs = out)
    if(train_slow):
        model.compile(optimizer = LRMultiplier('adam', {'slow':lr_s, 'fast':lr_f, 'slow_2':lr_s, 'fast_2':lr_f}),
                    loss='binary_crossentropy',
                    metrics=['accuracy'])
    else:
        slow.trainable = False
        s2.trainable = False
        model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.05,),
                      loss='binary_crossentropy',
                      metrics=['accuracy'])
    return model

def set_fs_weights(slow_model):
    fs_model = slow_fast_nn()
    sm_weights = slow_model.get_weights()
    fs_model.get_layer('slow').set_weights(sm_weights[:2])
    fs_model.get_layer('slow_2').set_weights(sm_weights[2:])
    return fs_model



    

### Supervised vs Self Supervised

# Supervised exposure
def s_expose(model, train, batch_size, epoch_size):
    hist = model.fit(
        train[0], train[1],
        batch_size = batch_size,
        epochs = epoch_size,
        validation_data=(train[2], train[3]),
        verbose = 0
    )
    return (model, hist)

# Create plots to compare weights of 2 models
def compare_weights_2(model, train, batch_size, epoch_size, id, xlab, ylab):
    
    # get all weights before exposing
    before_weight_dict = {}
    for layer in model.layers:
        name = layer.get_config()['name']
        if layer.get_weights() != []:
            weights = layer.get_weights()[0]
            before_weight_dict[name] = weights
    
    result = {}
    result["slow"] = []
    result["slow_2"] = []
    result["fast"] = []
    result["fast_2"] = []
    
    # almost no weight changes after 30th batch
    for i in range(0, 30):
        # expose model 1 batch
        model.fit(np.array([train[0][i]]), np.array([train[1][i]]),batch_size = 1, 
                  epochs = 1, verbose = 0)
    
        # get weights after exposing
        after_weight_dict = {}
        for layer in model.layers:
            name = layer.get_config()['name']
            if layer.get_weights() != []:
                weights = layer.get_weights()[0]
                after_weight_dict[name] = weights
    
        # add weights to result dictionary
        if (i % 1 == 0):
            for key in after_weight_dict:
                if key in result:
                    # trim weight array by "by" (we cant plot all 900 weights)
                    if ((key == "slow") or (key == "fast")):
                        by = 50
                    else: by = 1
                    result[key] = np.append(result[key], (after_weight_dict[key]).flatten()[::by])
                    
                #after = (after_weight_dict[key]).flatten()
                #before = (before_weight_dict[key]).flatten()
                #result[key] = result[key] + [np.sum(np.absolute(np.subtract(after,before)))]
    
    print("Exposure phase done")
    print("Start plot")
    # plot 3d weight change plot
    # x = batch
    # y = weight index
    # z = weight value
    
    # slow_1
    
    # batchLen = len(train[0])//10
    # weightLen = int(len(result["slow"])/batchLen)

    batchLen = 30
    weightLen = int(len(result["slow"])/batchLen)
    
# scatter 3d plot
   
    fig = plt.figure(figsize=plt.figaspect(0.5))
    
    x = np.repeat(np.arange(1,batchLen+1),weightLen)
    y = np.tile(np.arange(1, weightLen+1), batchLen)*50
    z_slow = result["slow"]
    z_fast = result["fast"]
   
    print (len(x), len(y), len(z_slow))
    ax = fig.add_subplot(1,2,1,projection = '3d')
    ax.scatter3D(x,y,z_slow,c=z_slow,cmap='Blues')
    ax.set_title(id+" exposure : Slow layer weights")
    ax.set_xlabel("Batch")
    ax.set_ylabel("Weight index")
    ax.set_zlabel("Weight value")
    
    ax = fig.add_subplot(1,2,2,projection = '3d')
    ax.scatter3D(x,y,z_fast,c=z_fast,cmap='Blues')
    ax.set_title(id+" exposure : Fast layer weights")
    ax.set_xlabel("Batch")
    ax.set_ylabel("Weight index")
    ax.set_zlabel("Weight value")
    plt.show()
    
    return
    
    
    
    
# Self supervised exposure


def test_d2_reliance(model, test_l, test_h):
    preds_l = model.predict(test_l) >= 0.5
    preds_h = model.predict(test_h) >= 0.5
    return (sum(preds_l)/len(preds_l), sum(preds_h)/len(preds_h))

# Create plots to compare weights of 2 models
def compare_weights(s_model, ss_model, id, xlab, ylab):
    s_weight_dict = {}
    for layer in s_model.layers:
        name = layer.get_config()['name']
        if layer.get_weights() != []:
            weights = layer.get_weights()[0]
            s_weight_dict[name] = weights
    
    ss_weight_dict = {}
    for layer in ss_model.layers:
        name = layer.get_config()['name']
        if layer.get_weights() != []:
            weights = layer.get_weights()[0]
            ss_weight_dict[name] = weights
            
    for key in s_weight_dict:
        plt.figure()
        x = s_weight_dict[key]
        y = ss_weight_dict[key]
        plt.scatter(x, y, s = 2)
        plt.xlabel(xlab)
        plt.ylabel(ylab)
        plt.title("Weights of "+key+" layer exposed on "+id)
        plt.savefig(id+"_"+key+".png")
    return
        
        
def compare_predictions(s_model, ss_model, test, id, xlab1, xlab2):
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.suptitle(id)
    ax1.scatter(s_model.predict(test[2]), test[3])
    ax2.scatter(ss_model.predict(test[2]), test[3])
    ax1.set_xlabel(xlab1)
    ax2.set_xlabel(xlab2)
    ax1.set_ylabel('actual label')
    plt.savefig(id+"_predictions.png")
    return


pretrain = loadOfInt("pretrain.pkl")
canonical = loadOfInt("canonical.pkl")
rev3 = loadOfInt("reverse.pkl")
low = loadOfInt("low_d2_test.pkl")
high = loadOfInt("high_d2_test.pkl")


EPOCHS = 10
BATCH_SIZE = 100

slow_model = ff_nn()
slow_hist = slow_model.fit(
    pretrain[0], pretrain[1],
    batch_size = BATCH_SIZE,
    epochs = EPOCHS,
    validation_data=(pretrain[2], pretrain[3])
)

init_canon = set_fs_weights(slow_model)
init_ss_canon = set_fs_weights(slow_model)

init_rev = set_fs_weights(slow_model)
init_ss_rev = set_fs_weights(slow_model)



compare_weights_2(init_canon, canonical, 1, 1, "Canonical","before expose","after expose")
compare_weights_2(init_rev, rev3, 1, 1, "Reversed","before expose","after expose")


